<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Virtual Reality Image Dataset vCAT Helps Research on Semantic Segmentation Algorithms">
  <meta name="keywords" content="VR, Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Virtual Reality Image Dataset vCAT Helps Research on Semantic Segmentation Algorithms</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/stanford.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand is-flex is-justify-content-center is-align-items-center" style="width: 100%;">
    <div class="has-text-centered is-size-5 has-text-weight-semibold">
      2022 IEEE International Conference on Industrial Technology (ICIT)
    </div>
  </div>
</nav>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">
            üêà  Virtual Reality Image Dataset vCAT Helps Research on Semantic Segmentation Algorithms</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Wenjie Li<sup>1</sup>, </span>
              <span class="author-block"><b>Yunxin Fan</b><sup>1</sup>, </span>
              <span class="author-block">Shugen Ma<sup>2</sup>, </span>
              <span class="author-block">Wenchuan Jia<sup>1,*</sup></span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>Shanghai Key Laboratory of Intelligent Manufacturing and Robotics, Shanghai, China</span>
              <span class="author-block"><sup>2</sup>Department of Robotics, Ritsumeikan University, Japan</span>
              <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <section class="framework">
      <div class="container is-max-desktop">
        <div class="hero-body has-text-centered">
          <img id="teaser-image" src="./static/images/segmentation_result.png" alt="Framework Image" width="90%">
          <p>
            Segmentation results with our framework and VR dataset.
          <!-- MathJax script for rendering LaTeX -->
          <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
          </script>
        </div>
      </div>
    </section>



    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-4">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                It is difficult to create sample datasets for semantic identification for animals with quick behavior traits, such as cats,
                 and the cost of dataset gathering is greatly increased by high-speed motion capture cameras. 

              </p>
              <p>
                In this work, we take the common Chinese dragon-li
                üêà  as an example, carries on the solid modeling and the gait movement design in the digital 3D modeling software, 
                then constructs the virtual image dataset vCAT through the image rendering technology. 
                To account for changing elements like the body shape, and scene and camera perspective, the vCAT dataset has numerous data sub-categories. 
                In addition, we accomplished the pixel-level segmentation test of synthetic images and actual scene images using deep learning on the vCAT dataset based on the attention mechanism. 
                During the basis of training, the recognition accuracy rate can reach <strong>84%</strong> when only <strong>726</strong> images with real scene modeling were used. 

              </p>
              <p>
                In addition to providing a fully automated technical method for creating the original image of the virtual image dataset with its corresponding labels,
                 we also thoroughly test and discuss elements like scenes, perspectives, textures,
                 and lighting in the virtual dataset to serve as a foundation for its practical use in deep learning training.

              </p>

            </div>
          </div>
        </div>
        <!--/ Abstract. -->
    
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      </div>
    </section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- üìå Features -->
    <div class="column is-full-width">
      <h2 class="title is-4">Dataset Collection and Annotation</h2>
      <div class="content has-text-justified">
        <div class="hero-body has-text-centered">        
          <img id="teaser-image" src="./static/images/3dsmax.png" alt="Framework Image" width="90%">
          <p>
            A virtual cat constructed based on the actual appearance of a Chinese 
            dragon-li in the 3dsMax environment. (a) The picture is a photo of a Chinese 
            dragon-li, and (b) is a virtual model based on (a) ,including posture and body 
            type of the Chinese dragon-li..
          </p>
        </div>

        <p>
          We modeled a cat in 3dsMax. Its appearance refers to the Chinese dragon-li
          in real life. During the modeling process, the characteristics
          of the cat are highlighted, such as the body size, posture, and
          the selected modeling object Chinese dragon-li. The dataset
          is produced by adjusting the different orientations of the
          software and the pictures are intercepted. Finally, a 2D image is optimized through the 3D model.
        </p>
        <p>
          Refer to the continuous motion of the cat when running or
          walking, we use the timeline animation
          tool to set the motion key frame of the cat model in the
          3dsMax to generate continuous motion, and under the preset
          camera perspective Perform rendering to get the virtual image
          information of the cat. Furthermore, by
          hiding the scene objects and setting the RGB channel in the
          image rendering tool, the two-category label of the picture
          can be directly obtained.
        </p>
        <p>
          In this paper, the dataset is divided according to the two
          types of content, the cat (foreground) and scene (background) in the images. 
          </p>
        <!-- MathJax script for rendering LaTeX -->
        <script type="text/javascript" async
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
      </div>
    </div>
    <br/>
  
        <!-- üìå Features -->
        <div class="column is-full-width">
          <h2 class="title is-4">Model Architecture and Training Details</h2>
          <div class="content has-text-justified">
            <div class="hero-body has-text-centered">        
              <img id="teaser-image" src="./static/images/model.png" alt="Framework Image" width="100%">
              <p>
                <strong>Segmentation Transformer (SETR)</strong> Model Architecture.
              </p>
            </div>
    
            <ul>
              <li>
                The model used in this paper is <strong>Segmentation Transformer (SETR)</strong>, which integrates a <strong>ViT (Vision Transformer)</strong> encoder with a decoder for semantic segmentation tasks.
              </li>
              <li>
                Instead of traditional FCNs, the transformer encoder captures <strong>global semantic information</strong>, enhancing learning efficiency.
              </li>
              <li>
                The input image is divided into <strong>16√ó16 patches</strong> and processed in parallel via <strong>multi-head attention</strong>, inspired by techniques in NLP.
              </li>
              <li>
                After flattening and adding a <strong>class token</strong>, the input becomes a sequence of shape: 
                (H/16 √ó W/16 + 1) √ó (C √ó 16 √ó 16).
              </li>
              <li>
                The sequence is passed through the decoder to perform <strong>image restoration and recognition</strong>.
              </li>
              <li>
                A <strong>sigmoid activation function</strong> is used for the binary classification task, mapping output values to the (0,1) interval.
              </li>
            </ul>
            <!-- MathJax script for rendering LaTeX -->
            <script type="text/javascript" async
              src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
            </script>
          </div>
        </div>
        <br/>

        <section class="section">
          <div class="container is-max-desktop">
        
            <!-- üìå Features -->
            <div class="column is-full-width">
              <h2 class="title is-4">Discussion and Conclusion</h2>
              <div class="content has-text-justified">
                <div class="hero-body has-text-centered">        
                  <img id="teaser-image" src="./static/images/segmentation_result.png" alt="Framework Image" width="100%">
                  <p>
                    Segmentation results.
                  </p>
                </div>
                <ul>
                  <li>
                    Developed the vCAT dataset to enhance semantic segmentation in deep learning.
                  </li>
                  <li>
                    Utilized 3D modeling and rendering for automated dataset generation, reducing costs and effort.
                  </li>
                  <li>
                    Achieved <strong>84% accuracy</strong> in pixel-wise segmentation using both synthetic and real images.
                  </li>
                  <li>
                    Demonstrated that combining synthetic and real data improves model generalization for vision tasks.
                  </li>
                </ul>

                <!-- MathJax script for rendering LaTeX -->
                <script type="text/javascript" async
                  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
                </script>
              </div>
            </div>
            <br/>
        </section>
        



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{SurgicalSimulation,
  author    = {Wenjie Li, Yunxin Fan, Shugen Ma, Wenchuan Jia},
  title     = {Virtual Reality Image Dataset vCAT Helps Research on Semantic Segmentation Algorithms},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We appreciate <a
              href="https://nerfies.github.io">Nerfies</a> for providing
            the template for this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
